# Support Pick Impact in League of Legends Esports
Author: Jayleen Shang


## Overview
This data science project conducted at UCSD in 2025 explores the impact of support pick type on match results in the game League of Legends.


## Introduction 
League of Legends (LOL) is a popular MOBA-style team game developed by Riot Games, with its Esports scene pulling in millions of concurrent online viewers per game.
Developed by Oracle’s Elixir, the dataset used in this study records information for every match played in professional LOL Esports during the 2022 season. It contains 150,588 rows and 164 columns, noting detailed team-match information including the players' champion choices, roles, performance metrics, and outcomes across top competitive regions and patches. 

The support role in this 5v5 team game is tasked with enabling the team's carries and providing allied players intel on enemy activity by obtaining vision around the arena map. Support-oriented characters come in all shapes and colors, but can be notably classified into several labels. In LOL professional play, we can focus more generally on two major umbrella groups: engagers and enchanters. 

The support meta in League of Legends shifts constantly, but professional teams often stick with a more stable, engage-oriented pool. 
Even in seasons where enchanters are considered strong (2022), pros frequently default to tank supports for several well-known reasons: 
1. Players develop long-term mastery on champions that they've played extra frequently like Nautilus or Rakan.
2. Tank supports can ward and control vision safely thanks to mobility or durability.
3. Drafting an engage support often frees the rest of the team to run damage-focused or carry-heavy compositions.
Despite this, enchanters often show higher win rates on paper, raising the question of whether the comfort-driven 
engage meta is actually optimal.

The central question of this project is:

**Do engage/tank supports or enchanter supports lead to better team success in professional play?**

This study explores into the concept: if engage supports are so favored in pro play, why do they appear to win less often than enchanters? Understanding this balance may give insight into drafting decisions, team strategy, and the potential of a wide variety of "support" that is available to pick from in a competitive setting.

Columns from the dataset that we will focus on for this analysis include:
| Column        | Description                                                       |
| -----------   | -----------                                                       |
| `gameid`      | A unique game identifier                                          |
| `league`      | The competitive league/tournament name (e.g., LCS, LEC, LCK, LPL) | 
| `position`    | Player's role (Top, Jungle, Mid, ADC, Support) or team-level data | 
| `champion`    | The champion name selected by the player                          | 
| `result`      | The match result (binary: 1 = win, 0 = loss)                      | 
| `gamelength`  | Game duration in seconds                                          | 
| `kills`       | Total number of kills by the player                               | 
| `deaths`      | Total number of deaths by the player                              | 
| `assists`     | Total number of assists by the player                             | 
| `assistsat20` | Number of assists by 20 minutes into the game                     | 
| `visionscore` | Vision score (measure of vision control contribution)             | 


## Data Cleaning and Exploratory Data Analysis
### Data Cleaning
To make our analysis of the dataset more efficient and convenient, we conduct the following data cleaning steps:

1. Isolate only support-position player, because each row represents an individual player (or team total) and supports are the focus of analysis for this project. Drop the `position` column after.
2. Keep only the columns the set labeled above, containing match identifiers, champion names, position, win/loss results, K/DA, etc. 
3. Replace placeholder values such as `None`, `-1`, and empty strings with NaN, since they represent missing data generated by the OracleElixir scraping process. 
4. Drop rows missing champion names or results, and filter out extremely short matches (remakes). 
5. Create a new column (`support_type`) that classifies each support pick as either an engage support or an enchanter support, based on common pro play archetypes. The non-conforming support picks are filtered out. 

All support champs in the final dataset listed below:
- **ENGAGE:** 'Alistar', 'Amumu', 'Blitzcrank', 'Braum', 'Leona', 'Nautilus', 'Pyke', 'Rakan', 'Rell', 'Thresh'
- **ENCHANTER:** 'Janna', 'Karma', 'Lulu', 'Nami', 'RenataGlasc', 'Seraphine',  'Sona', 'Soraka', 'Yuumi'

The first five rows of the cleaned dataset are shown below:
| gameid                | league   | champion | result | gamelength | visionscore | kills | deaths | assists | assistsat20 |
|:----------------------|:---------|:---------|--------|------------|-------------|-------|--------|---------|-------------|
| ESPORTSTMNT01_2690210 | LCKC     | Leona    |   0    |       1713 |        69.0 |     1 |      5 |       6 |         2.0 |
| ESPORTSTMNT01_2690210 | LCKC     | Alistar  |   1    |       1713 |        67.0 |     0 |      2 |      18 |         7.0 |
| ESPORTSTMNT01_2690219 | LCKC     | Rakan    |   0    |       2114 |       109.0 |     0 |      3 |       2 |         0.0 |
| ESPORTSTMNT01_2690219 | LCKC     | Leona    |   1    |       2114 |       123.0 |     0 |      1 |      12 |         3.0 |
| 8401-8401_game_1      | LPL      | Nautilus |   1    |       1365 |        38.0 |     1 |      4 |       7 |         NaN |

### Univariate Analysis
For exploratory data analysis, we first examine the most picked support champions.
<iframe
  src="assets/uni_fig1.html"
  width="800"
  height="500"
  frameborder="0"
></iframe>
Nautilus dominated picks, with fellow tank engage champs Leona, Rakan, and Thresh following close behind. Enchanters like Janna, Seraphine, or Nami have much fewer games; the last four ranks by game count are enchanter champions. This reflects the pro meta in 2022, where hard-engage supports were favored for roaming and teamfight setup.

Next, let's take a look at the distribution of vision scores across players and games. 
<iframe
  src="assets/uni_fig2.html"
  width="800"
  height="500"
  frameborder="0"
></iframe>
Most supports likely cluster in a moderate vision score range (~60-90), with some outliers having extremely high vision scores, mostly likely from particularly long games. The large spread shows that pro supports prioritize vision control, but the exact amount (scaling with game length) potentially ranges depending on champion mobility and playstyle.

### Bivariate Analysis
Now, we'll compare the differences in vision scores between the two support types.
<iframe
  src="assets/bi_fig1.html"
  width="800"
  height="500"
  frameborder="0"
></iframe>
Engage/tank supports consistently have higher vision scores than enchanters. This makes sense because engage supports tend to roam more and have higher mobility or tankiness, allowing them to safely place wards and control vision throughout the map.

Examining the kill and assist counts for both support types:
<iframe
  src="assets/bi_fig2.html"
  width="800"
  height="500"
  frameborder="0"
></iframe>
Enchanters typically have low kills and an average number of assists (comparatively), with some outliers for both metrics. Engage supports, on the other hand, have a slightly wider spread in kills. This reflects the aggressive, fight-starting role of engage supports versus the protective, follow-up role of enchanters. We may also note that the engager outliers with very high kills all have very low assists, indicating a more carry-oriented playstyle on certain engage supports.

### Interesting Aggregates
We uncovered some mean statistics for each support type, with result representing winrate:

| Support Type | Kills    | Deaths   | Assists  | Vision Score | Result   |
|--------------|----------|----------|----------|--------------|----------|
| Engage       | 0.792907 | 3.619333 | 9.097860 | 83.245791    | 0.502619 |
| Enchanter    | 0.854053 | 2.534136 | 9.671431 | 73.878292    | 0.472635 |

Here are the mean assists and kills by champion: 

| Champion    | Type      | Avg Assists | Avg Kills |
|-------------|-----------|-------------|-----------|
| Pyke        | Engage    | 5.582456    | 3.350877  |
| Blitzcrank  | Engage    | 9.222749    | 1.075829  |
| Amumu       | Engage    | 9.438753    | 0.917595  |
| Rakan       | Engage    | 10.138614   | 0.776446  |
| Rell        | Engage    | 9.983146    | 0.758427  |
| Nautilus    | Engage    | 8.660550    | 0.757339  |
| Leona       | Engage    | 8.644696    | 0.734789  |
| Thresh      | Engage    | 9.771429    | 0.732046  |
| Alistar     | Engage    | 9.457978    | 0.634592  |
| Braum       | Engage    | 9.469259    | 0.532550  |
| Janna       | Enchanter | 9.207317    | 0.402439  |
| Karma       | Enchanter | 8.626153    | 0.930427  |
| Lulu        | Enchanter | 8.932191    | 0.643826  |
| Nami        | Enchanter | 11.130520   | 1.016683  |
| Seraphine   | Enchanter | 9.294118    | 1.367647  |
| Sona        | Enchanter | 9.954023    | 0.850575  |
| Soraka      | Enchanter | 8.458904    | 0.438356  |
| Yuumi       | Enchanter | 10.568776   | 0.880169  |

Engage/tank supports have higher average vision scores and take more risks in fights, resulting in slightly higher deaths and frontline presence, while enchanters stay safer with more consistent assists. Pyke, as an assassin-style engage support, is an outlier that drives up the engage kills average.

## Assessment of Missingness
### NMAR Analysis
Looking through the full dataset, the columns `ban1` through `ban5` are most likely to be Not Missing At Random (NMAR). In League of Legends esports matches, players sometimes choose not to ban a champion or run out of time to submit a ban, which directly causes missing values. These missing entries are not related to any other variables in the dataset, so the missingness is inherently tied to the decision-making or timing process itself. To potentially make this missingness MAR instead of NMAR, we would need additional data such as timestamps for when each ban was submitted or team strategy notes indicating intentional non-bans.

### Missingness Dependency
Let's investigate whether the missingness of `assistsat20` depends on other variables in the dataset. We'll use two permutation 
tests using a significance level of 0.05 and Total Variation Distance (TVD) as the test statistic, comparing the distribution of `league` and `result` between rows where `assistsat20` is missing and where it is not missing. 

- **Null Hypothesis (H₀):**
The distribution of `league` is the same when `assistsat20` is missing vs not missing.
- **Alternative Hypothesis (H₁):**
The distribution of `league` is different when `assistsat20` is missing vs not missing.
<iframe
  src="assets/emp_fig1.html"
  width="800"
  height="500"
  frameborder="0"
></iframe>
We found the observed **TVD: 0.9741**, and after permutation testing, the **p-value: 0.0**. Since the observed TVD is far from the data and the p-value is effectively 0, we reject the null hypothesis. This provides strong evidence that the missingness of `assistsat20` depends on the league in which the match was played.

Now, to test `assistsat20` with `result`:
- **Null Hypothesis (H₀):** The distribution of `result` is the same when `assistsat20` is missing vs not missing.
- **Alternative Hypothesis (H₁):** The distribution of `result` is different when `assistsat20` is missing vs not missing.
<iframe
  src="assets/emp_fig2.html"
  width="800"
  height="500"
  frameborder="0"
></iframe>
We found that the observed **TVD: 0.0**, and post-permutation test, the **p-value: 0.907**.

Since the observed TVD aligned with the data and the p-value is greater than the 0.5 significance level, we fail to reject the null hypothesis. This indicates no meaningful difference in match outcomes based on the missingness of `assistsat20`. Therefore, missingness of `assistsat20` does not depend on result.

## Hypothesis Testing
Next, we'll test whether teams that field engage supports have a higher win rate than teams that field Enchanter supports in tier one LoL esports matches.

- **Null hypothesis(H₀):** The distribution of win rate for teams with engage supports is the same as the distribution of win rate for teams with enchanter supports. Any observed difference is due to random chance.
- **Alternative hypothesis (H₁):** Teams with engage supports have a higher win rate than teams with enchanter supports.

The test statistic used is the difference in mean win rate between support types (engage and enchanter) as classified in the `support_type` column, with 0.05 as the significance level. Win rate is naturally summarized by group means (proportion wins), and the difference of means is intuitive and straightforward to interpret. A permutation test on team-level win labels retains all other structure and makes minimal assumptions about distribution, which is good for pro play data where outcomes are complex.
<iframe
  src="assets/hypo_fig.html"
  width="800"
  height="500"
  frameborder="0"
></iframe>
The observed difference in mean win rate between teams with engage supports and enchanter supports is approximately 0.03, indicating that engage supports have a 3% higher win rate on average. The permutation test yielded a **p-value of 0.0004** for the one-sided test, which is well below the significance level of 0.05. This provides strong evidence to reject the null hypothesis in favor of the alternative. Therefore, we conclude that teams fielding engage supports have a statistically significantly higher win rate than teams fielding enchanter supports in tier one LoL esports matches in 2022.

## Framing a Prediction Problem
Our prediction task focuses on determining whether a professional League of Legends support player’s team will win or lose a match (`result`). This is a binary classification problem, since the response variable takes on two outcomes: win (1) or loss (0). We chose `result` because winning is the central outcome teams care about, and evaluating how support performance influences match results contribute to that meaning in a less typical way.

Our baseline model is a Random Forest Classifier implemented inside a single scikit-learn Pipeline. The Pipeline ensures that all preprocessing steps occur consistently: numerical features are scaled, and categorical features are encoded before being passed to the model. 

Our baseline model uses two quantitative features and one nominal categorical feature:
- `support_type`: indicating whether a support is an engage or enchanter champion, encoded using One-Hot Encoding
- `visionscore`: giving a numeric measure of map vision impact, processed with StandardScaler
- `assists`: reflecting involvement in combat and team fights, also scaled with StandardScaler

We evaluate our model with the F1-score, which balances precision and recall. Since the dataset may not be perfectly balanced between wins and losses, F1 avoids misleadingly high accuracy that could come from predicting the majority class and instead emphasizes meaningful predictive performance.

## Baseline Model
For the baseline model, I used sklearn pipeline with a Random Forest Classifier to predict whether a tier-one professional League of Legends support player’s team in 2022 would win or lose a match. The model includes three features: two quantitative features being `visionscore` and `assists`, and one nominal feature, `support_type` (engage/tank vs. enchanter). To prepare the data for analysis, the numeric features were standardized using a StandardScaler, and the nominal feature was encoded using OneHotEncoder.

**Model Performance:**
- **Baseline Accuracy: 0.80**
- **F1-score: 0.80**, balanced across both classes (win/loss)

Results Interpretation:

The model performs reasonably well, indicating it generalizes decently to unseen data with an accuracy of 80% and a very similar F1-score. This performance is considered 'alright', but there is always room for improvement, particularly with including more situational interactions between features (e.g., differences between engage and enchanter supports in different team compositions). Factors such as the choice of features, the type of model, and the handling of missing data could be contributing to its limitations.

To enhance the model's performance, we could consider feature engineering to better capture key interactions of the support role's game. We could also apply hyperparameter tuning to optimize the performance of the decision tree.

## Final Model
For the Final Model, I built upon the baseline Random Forest Classifier by adding three engineered features:
- `visionscore_per_sec` (`visionscore` / `gamelength`): captures how efficiently a support player converts game time into vision control, reflecting roaming and map awareness
- `assistsat20_vs_min` (`assistsat20` / (`gamelength` / 60)): measures how active a support is in the first 20 minutes relative to game pace, highlighting early influence in fights
- `KDA_ratio` (`kills` + `assists`) / (`deaths` + 1): summarizes overall battle contribution, protected against division by zero
These features were chosen based on the data generating process: professional supports contribute to their team through vision control, early-game play, and participation in skirmishes. Quantifying these contributions allows the model to better capture patterns predictive of match outcomes.

I retained the original numeric features (`visionscore`, `assists`) and the categorical feature `support_type` (engage vs. enchanter), which was encoded via OneHotEncoder. All numeric features, including the engineered ones, were standardized using StandardScaler to ensure fair weighting in the model.

The Random Forest Classifier was used again due to its ability to handle non-linear interactions, and interpretability of feature importance. Hyperparameters were tuned using GridSearchCV with 5-fold cross-validation, optimizing for F1-score to balance precision and recall. The best hyperparameters selected were:
- n_estimators = 200
- max_depth = 20
- min_samples_split = 5

**Model Performance:**
- **Final Model Accuracy: 0.89**
- **F1-Score: 0.89** for both win and loss classes, with balanced precision and recall

Compared to the Baseline Model (accuracy 0.80, F1-score 0.80), the Final Model shows a substantial improvement. This improvement is consistent with expectations from the data generating process; the engineered features provide useful summaries of early-game activity, fight contribution, and vision efficiency, which are critical factors in professional support play. With more attention to detailed interactions like these, the Random Forest can better distinguish likely winners from losers, resulting in higher predictive performance.

## Fairness Analysis
For our analysis on fairness, lets revisit engage vs. enchanger again. We create a new column `engage_enchanter` that functions as a one-hot encoded variable, where engage/tank supports (`engage_enchanter == 1`) and enchanter supports (`engage_enchanter == 0`).

The evaluation metric will be Precision for the positive class (predicting result == 1, i.e., wins). Precision is chosen because we care about how often predicted wins are correct for each group. We'll go with difference in precision (Precision_engage − Precision_enchanter) for the test statistic, and the significance level will remain α = 0.05.

- **Null Hypothesis (H₀):** The model is fair: precision for Engage supports equals precision for Enchanter supports; any observed difference is due to random chance.
- **Alternative Hypothesis (H₁):** The model is unfair: precision for Engage supports is lower than precision for Enchanter supports.
<iframe
  src="assets/fair_fig.html"
  width="800"
  height="500"
  frameborder="0"
></iframe>

**Result:**

Observed precision (Engage) = 0.8828 - Observed precision (Enchanter) = 0.8623 = **Observed difference = 0.025**

**Permutation test p-value = 0.0272**

Since the p_value is less than 0.05 we reject the null. The model shows statistically significant lower precision for engage/tank supports compared to enchanter supports. This suggests unfairness: the model makes more incorrect 'win' predictions for engage supports (relative to enchanter) than would be expected by chance. 

The histogram shows the observed difference in precision (engage minus enchanter) line is on the left tail of the distribution, indicating that engage supports have lower precision than enchanters more often than would be expected by chance. This suggests that the model performs slightly worse when predicting game outcomes for engage supports, pointing to a potential fairness concern for that group.